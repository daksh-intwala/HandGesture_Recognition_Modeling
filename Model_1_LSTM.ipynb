{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Importing Libraries\n"
      ],
      "metadata": {
        "id": "ghRlRGQOSafW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJGNKTVCSMCx",
        "outputId": "48af018e-54f2-40ab-c897-af90ac79695c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.9/dist-packages (0.9.2.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (3.20.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from mediapipe) (1.22.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.9/dist-packages (from mediapipe) (4.7.0.72)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (23.3.3)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (22.2.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (5.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (1.4.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (23.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (3.0.9)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (8.4.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->mediapipe) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras_preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from keras_preprocessing) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from keras_preprocessing) (1.22.4)\n",
            "Installing collected packages: keras_preprocessing\n",
            "Successfully installed keras_preprocessing-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install mediapipe\n",
        "import mediapipe as mp\n",
        "from google.colab import files\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "!pip install keras_preprocessing\n",
        "from keras_preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Working on the Data"
      ],
      "metadata": {
        "id": "87vfnEzZWgLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiFy_lK3Sjf4",
        "outputId": "778c15e3-3a5e-4726-b1ad-dd339728d027"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extracting landmarks"
      ],
      "metadata": {
        "id": "PYAY1Dq7Wm4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def landmark_extractor(img):\n",
        "\n",
        "  mp_hands = mp.solutions.hands\n",
        "\n",
        "  with mp_hands.Hands(static_image_mode=True,\n",
        "                      max_num_hands=2,\n",
        "                      min_detection_confidence=0.5) as hands:\n",
        "      \n",
        "        landmark = []\n",
        "\n",
        "        image = cv2.flip(cv2.imread(img), 1)\n",
        "\n",
        "        results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        landmarks = results.multi_hand_landmarks\n",
        "        \n",
        "        if landmarks != None:\n",
        "          landmark_idx = mp_hands.HandLandmark.WRIST.value\n",
        "          landmark += (np.array([[lmk.landmark[landmark_idx].x, \n",
        "                        lmk.landmark[landmark_idx].y,\n",
        "                        lmk.landmark[landmark_idx].z] for lmk in landmarks]).flatten() if landmarks else np.zeros(3)).tolist()\n",
        "\n",
        "          landmark_idx = mp_hands.HandLandmark.THUMB_CMC.value\n",
        "          landmark += (np.array([[lmk.landmark[landmark_idx].x, \n",
        "                        lmk.landmark[landmark_idx].y, \n",
        "                        lmk.landmark[landmark_idx].z] for lmk in landmarks]).flatten() if landmarks else np.zeros(3)).tolist()\n",
        "\n",
        "          landmark_idx = mp_hands.HandLandmark.THUMB_MCP.value\n",
        "          landmark += (np.array([[lmk.landmark[landmark_idx].x, \n",
        "                        lmk.landmark[landmark_idx].y, \n",
        "                        lmk.landmark[landmark_idx].z] for lmk in landmarks]).flatten() if landmarks else np.zeros(3)).tolist()       \n",
        "\n",
        "          landmark_idx = mp_hands.HandLandmark.THUMB_IP.value\n",
        "          landmark += (np.array([[lmk.landmark[landmark_idx].x, \n",
        "                        lmk.landmark[landmark_idx].y, \n",
        "                        lmk.landmark[landmark_idx].z] for lmk in landmarks]).flatten() if landmarks else np.zeros(3)).tolist()   \n",
        "\n",
        "          landmark_idx = mp_hands.HandLandmark.THUMB_TIP.value\n",
        "          landmark += (np.array([[lmk.landmark[landmark_idx].x, \n",
        "                        lmk.landmark[landmark_idx].y, \n",
        "                        lmk.landmark[landmark_idx].z] for lmk in landmarks]).flatten() if landmarks else np.zeros(3)).tolist()\n",
        "\n",
        "          landmark_idx = mp_hands.HandLandmark.INDEX_FINGER_MCP.value\n",
        "          landmark += (np.array([[lmk.landmark[landmark_idx].x, \n",
        "                        lmk.landmark[landmark_idx].y, \n",
        "                        lmk.landmark[landmark_idx].z] for lmk in landmarks]).flatten() if landmarks else np.zeros(3)).tolist()\n",
        "\n",
        "          landmark_idx = mp_hands.HandLandmark.INDEX_FINGER_PIP.value\n",
        "          landmark += (np.array([[lmk.landmark[landmark_idx].x, \n",
        "                        lmk.landmark[landmark_idx].y, \n",
        "                        lmk.landmark[landmark_idx].z] for lmk in landmarks]).flatten() if landmarks else np.zeros(3)).tolist()\n",
        "\n",
        "          landmark_idx = mp_hands.HandLandmark.INDEX_FINGER_DIP.value\n",
        "          landmark += (np.array([[lmk.landmark[landmark_idx].x, \n",
        "                        lmk.landmark[landmark_idx].y, \n",
        "                        lmk.landmark[landmark_idx].z] for lmk in landmarks]).flatten() if landmarks else np.zeros(3)).tolist()       \n",
        "\n",
        "          landmark_idx = mp_hands.HandLandmark.INDEX_FINGER_TIP.value\n",
        "          landmark += (np.array([[lmk.landmark[landmark_idx].x, \n",
        "                        lmk.landmark[landmark_idx].y, \n",
        "                        lmk.landmark[landmark_idx].z] for lmk in landmarks]).flatten() if landmarks else np.zeros(3)).tolist()   \n",
        "\n",
        "          landmark_idx = mp_hands.HandLandmark.MIDDLE_FINGER_MCP.value\n",
        "          landmark += (np.array([[lmk.landmark[landmark_idx].x, \n",
        "                        lmk.landmark[landmark_idx].y, \n",
        "                        lmk.landmark[landmark_idx].z] for lmk in landmarks]).flatten() if landmarks else np.zeros(3)).tolist()\n",
        "\n",
        "          landmark_idx = mp_hands.HandLandmark.MIDDLE_FINGER_PIP.value\n",
        "          landmark += (np.array([[lmk.landmark[landmark_idx].x, \n",
        "                        lmk.landmark[landmark_idx].y, \n",
        "                        lmk.landmark[landmark_idx].z] for lmk in landmarks]).flatten() if landmarks else np.zeros(3)).tolist()\n",
        "\n",
        "          landmark_idx = mp_hands.HandLandmark.MIDDLE_FINGER_DIP.value\n",
        "          landmark += (np.array([[lmk.landmark[landmark_idx].x, \n",
        "                        lmk.landmark[landmark_idx].y, \n",
        "                        lmk.landmark[landmark_idx].z] for lmk in landmarks]).flatten() if landmarks else np.zeros(3)).tolist()\n",
        "\n",
        "          landmark_idx = mp_hands.HandLandmark.MIDDLE_FINGER_TIP.value\n",
        "          landmark += (np.array([[lmk.landmark[landmark_idx].x, \n",
        "                        lmk.landmark[landmark_idx].y, \n",
        "                        lmk.landmark[landmark_idx].z] for lmk in landmarks]).flatten() if landmarks else np.zeros(3)).tolist()       \n",
        "\n",
        "          landmark_idx = mp_hands.HandLandmark.RING_FINGER_MCP.value\n",
        "          landmark += (np.array([[lmk.landmark[landmark_idx].x, \n",
        "                        lmk.landmark[landmark_idx].y, \n",
        "                        lmk.landmark[landmark_idx].z] for lmk in landmarks]).flatten() if landmarks else np.zeros(3)).tolist()   \n",
        "\n",
        "          landmark_idx = mp_hands.HandLandmark.RING_FINGER_PIP.value\n",
        "          landmark += (np.array([[lmk.landmark[landmark_idx].x, \n",
        "                        lmk.landmark[landmark_idx].y, \n",
        "                        lmk.landmark[landmark_idx].z] for lmk in landmarks]).flatten() if landmarks else np.zeros(3)).tolist()\n",
        "\n",
        "          landmark_idx = mp_hands.HandLandmark.RING_FINGER_DIP.value\n",
        "          landmark += (np.array([[lmk.landmark[landmark_idx].x, \n",
        "                        lmk.landmark[landmark_idx].y, \n",
        "                        lmk.landmark[landmark_idx].z] for lmk in landmarks]).flatten() if landmarks else np.zeros(3)).tolist()\n",
        "\n",
        "          landmark_idx = mp_hands.HandLandmark.RING_FINGER_TIP.value\n",
        "          landmark += (np.array([[lmk.landmark[landmark_idx].x, \n",
        "                        lmk.landmark[landmark_idx].y, \n",
        "                        lmk.landmark[landmark_idx].z] for lmk in landmarks]).flatten() if landmarks else np.zeros(3)).tolist()\n",
        "\n",
        "          landmark_idx = mp_hands.HandLandmark.PINKY_MCP.value\n",
        "          landmark += (np.array([[lmk.landmark[landmark_idx].x, \n",
        "                        lmk.landmark[landmark_idx].y, \n",
        "                        lmk.landmark[landmark_idx].z] for lmk in landmarks]).flatten() if landmarks else np.zeros(3)).tolist()       \n",
        "\n",
        "          landmark_idx = mp_hands.HandLandmark.PINKY_PIP.value\n",
        "          landmark += (np.array([[lmk.landmark[landmark_idx].x, \n",
        "                        lmk.landmark[landmark_idx].y, \n",
        "                        lmk.landmark[landmark_idx].z] for lmk in landmarks]).flatten() if landmarks else np.zeros(3)).tolist()   \n",
        "\n",
        "          landmark_idx = mp_hands.HandLandmark.PINKY_DIP.value \n",
        "          landmark += (np.array([[lmk.landmark[landmark_idx].x, \n",
        "                        lmk.landmark[landmark_idx].y, \n",
        "                        lmk.landmark[landmark_idx].z] for lmk in landmarks]).flatten() if landmarks else np.zeros(3)).tolist()\n",
        "\n",
        "          landmark_idx = mp_hands.HandLandmark.PINKY_TIP.value\n",
        "          landmark += (np.array([[lmk.landmark[landmark_idx].x, \n",
        "                        lmk.landmark[landmark_idx].y, \n",
        "                        lmk.landmark[landmark_idx].z] for lmk in landmarks]).flatten() if landmarks else np.zeros(3)).tolist()\n",
        "\n",
        "        if len(landmark) == 126:\n",
        "            row = np.around(landmark, decimals=5).reshape(1, -1)\n",
        "        else:\n",
        "            row = np.hstack((np.array(landmark), np.zeros(126-len(landmark)))).reshape(1, -1)\n",
        "        return row"
      ],
      "metadata": {
        "id": "0cREQQZuTGyN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting video clip to frames\n",
        "\n",
        "def getFrame(vid_path, sec):\n",
        "  vid = cv2.VideoCapture(vid_path)\n",
        "  vid.set(cv2.CAP_PROP_POS_MSEC, sec*1000)\n",
        "  ret, image = vid.read()\n",
        "  path = vid_path[:-9]+\"frames/\"+vid_path[-9:-4]+f\"{sec}.jpg\"\n",
        "  if ret:\n",
        "    cv2.imwrite(path, image)\n",
        "\n",
        "    return path\n",
        "\n",
        "\n",
        "def gen_frames(vid_path, vid_length=2, frames_per_sec=15):\n",
        "\n",
        "  n_iterations = vid_length * frames_per_sec\n",
        "  frame_rate = 1 / frames_per_sec\n",
        "  sec = 0\n",
        "  frame_limit = 31\n",
        "  dataset = np.empty((126)).reshape(1, -1)\n",
        "  lst = [np.zeros(126)]\n",
        "\n",
        "  for i in range(int(n_iterations)):\n",
        "    sec += frame_rate\n",
        "    sec = round(sec, 3)\n",
        "    image = getFrame(vid_path, sec)\n",
        "    if image:\n",
        "      data_point = landmark_extractor(image)\n",
        "      if len(data_point)<= frame_limit:\n",
        "        dataset = np.append(dataset, data_point, axis=0)\n",
        "      else :\n",
        "        to_round = math.ceil(len(data_point)/frame_limit)\n",
        "        dataset = np.append(data_point[::to_round])\n",
        "  while len(dataset) <frame_limit:\n",
        "    dataset = np.append(dataset,lst,axis=0)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "brkZTwabTLWo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n8_QkkuQWrp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating Train Dataset"
      ],
      "metadata": {
        "id": "M-umlQBJWsPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_files = ['/content/drive/MyDrive/Datasets/Capstone/chair/09848.mp4',\n",
        "               '/content/drive/MyDrive/Datasets/Capstone/chair/09849.mp4',\n",
        "               '/content/drive/MyDrive/Datasets/Capstone/chair/09850.mp4',\n",
        "               '/content/drive/MyDrive/Datasets/Capstone/chair/09869.mp4'\n",
        "               ]\n",
        "\n",
        "action_1 = np.empty((126)).reshape(1, -1)\n",
        "action_1= np.delete(action_1,np.s_[:],axis=0)\n",
        "for video in video_files:\n",
        "  int_data1 = gen_frames(video)\n",
        "  print(\"int-data\",len(int_data1))\n",
        "  action_padded = pad_sequences(int_data1, padding='post', maxlen=126, dtype='float32')\n",
        "  print('action_padded',len(action_padded))\n",
        "  action_1 = np.append(action_1, int_data1, axis=0)\n",
        "  print(\"action_1\",len(action_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa_ApnNFTSoP",
        "outputId": "17a899d4-6358-4e53-aa64-8eb11323a878"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "int-data 31\n",
            "action_padded 31\n",
            "action_1 31\n",
            "int-data 31\n",
            "action_padded 31\n",
            "action_1 62\n",
            "int-data 31\n",
            "action_padded 31\n",
            "action_1 93\n",
            "int-data 31\n",
            "action_padded 31\n",
            "action_1 124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_files = ['/content/drive/MyDrive/Datasets/Capstone/computer/12311.mp4',\n",
        "               '/content/drive/MyDrive/Datasets/Capstone/computer/12313.mp4',\n",
        "               '/content/drive/MyDrive/Datasets/Capstone/computer/12328.mp4',\n",
        "               '/content/drive/MyDrive/Datasets/Capstone/computer/12338.mp4'\n",
        "               ]\n",
        "\n",
        "action_2 = np.empty((126)).reshape(1, -1)\n",
        "action_2= np.delete(action_2,np.s_[:],axis=0)\n",
        "for video in video_files:\n",
        "  int_data2 = gen_frames(video)\n",
        "  print(\"int-data\",len(int_data2))\n",
        "  #print(int_data2)\n",
        "  action_padded = pad_sequences(int_data2, padding='post', maxlen=126, dtype='float32')\n",
        "  print('action_padded',len(action_padded))\n",
        "  action_2 = np.append(action_2, action_padded, axis=0)\n",
        "  print(\"action_2\",len(action_2))\n",
        "  #print(action_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utttxppTVpbK",
        "outputId": "12edd986-2d79-4a46-b989-a628d6f5de67"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "int-data 31\n",
            "action_padded 31\n",
            "action_2 31\n",
            "int-data 31\n",
            "action_padded 31\n",
            "action_2 62\n",
            "int-data 31\n",
            "action_padded 31\n",
            "action_2 93\n",
            "int-data 31\n",
            "action_padded 31\n",
            "action_2 124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_files = ['/content/drive/MyDrive/Datasets/Capstone/drink/17710.mp4',\n",
        "               '/content/drive/MyDrive/Datasets/Capstone/drink/17734.mp4',\n",
        "               '/content/drive/MyDrive/Datasets/Capstone/drink/65539.mp4',\n",
        "               '/content/drive/MyDrive/Datasets/Capstone/drink/69302.mp4'\n",
        "               ]\n",
        "\n",
        "action_3 = np.empty((126)).reshape(1, -1)\n",
        "action_3= np.delete(action_3,np.s_[:],axis=0)\n",
        "for video in video_files:\n",
        "  int_data3 = gen_frames(video)\n",
        "  print(\"int-data\",len(int_data3))\n",
        "  action_padded = pad_sequences(int_data3, padding='post', maxlen=126, dtype='float32')\n",
        "  print('action_padded',len(action_padded))\n",
        "  action_3 = np.append(action_3, action_padded, axis=0)\n",
        "  print(\"action_3\",len(action_3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSxyEXspV71h",
        "outputId": "842ff3bf-2870-4ee9-a642-db50327cdd4b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "int-data 31\n",
            "action_padded 31\n",
            "action_3 31\n",
            "int-data 31\n",
            "action_padded 31\n",
            "action_3 62\n",
            "int-data 31\n",
            "action_padded 31\n",
            "action_3 93\n",
            "int-data 31\n",
            "action_padded 31\n",
            "action_3 124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.append(action_1, action_2, axis=0)\n",
        "x_train = np.append(x_train, action_3, axis=0)"
      ],
      "metadata": {
        "id": "lC8ZOOgfWX7J"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ8HMCsIWZ22",
        "outputId": "66d32415-7a19-4957-ec46-19a7d28d4b66"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(372, 126)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y = []\n",
        "for i in range(0,12):\n",
        "  train_y = np.append(train_y,1).astype(int)\n",
        "for i in range(0,12):\n",
        "  train_y = np.append(train_y,2).astype(int)\n",
        "for i in range(0,12):\n",
        "  train_y = np.append(train_y,3).astype(int)"
      ],
      "metadata": {
        "id": "5lw47sA-WMDB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHlT-Ht3WRaf",
        "outputId": "46252a63-9ce3-4b2f-bd9c-a919b8abbfa5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36,)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating Testing Dataset"
      ],
      "metadata": {
        "id": "gpT6gxQvW4ZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting video clip to frames\n",
        "\n",
        "def getFrame(vid_path, sec):\n",
        "  vid = cv2.VideoCapture(vid_path)\n",
        "  vid.set(cv2.CAP_PROP_POS_MSEC, sec*1000)\n",
        "  ret, image = vid.read()\n",
        "  path = vid_path[:-9]+\"frames_test/\"+vid_path[-9:-4]+f\"{sec}.jpg\"\n",
        "  if ret:\n",
        "    cv2.imwrite(path, image)\n",
        "\n",
        "    return path\n",
        "\n",
        "\n",
        "def gen_frames(vid_path, vid_length=2, frames_per_sec=15):\n",
        "\n",
        "  n_iterations = vid_length * frames_per_sec\n",
        "  frame_rate = 1 / frames_per_sec\n",
        "  sec = 0\n",
        "  frame_limit = 31\n",
        "  dataset = np.empty((126)).reshape(1, -1)\n",
        "  lst = [np.zeros(126)]\n",
        "\n",
        "  for i in range(int(n_iterations)):\n",
        "    sec += frame_rate\n",
        "    sec = round(sec, 3)\n",
        "    image = getFrame(vid_path, sec)\n",
        "    if image:\n",
        "      data_point = landmark_extractor(image)\n",
        "      if len(data_point)<= frame_limit:\n",
        "        dataset = np.append(dataset, data_point, axis=0)\n",
        "      else :\n",
        "        to_round = math.ceil(len(data_point)/frame_limit)\n",
        "        dataset = np.append(data_point[::to_round])\n",
        "  while len(dataset) <frame_limit:\n",
        "    dataset = np.append(dataset,lst,axis=0)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "SFkkW0pwW7oC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_files = ['/content/drive/MyDrive/Datasets/Capstone/chair/09851.mp4',\n",
        "               ]\n",
        "\n",
        "action_1_test = np.empty((126)).reshape(1, -1)\n",
        "action_1_test= np.delete(action_1,np.s_[:],axis=0)\n",
        "for video in video_files:\n",
        "  int_data1 = gen_frames(video)\n",
        "  print(\"int-data\",len(int_data1))\n",
        "  action_padded = pad_sequences(int_data1, padding='post', maxlen=126, dtype='float32')\n",
        "  print('action_padded',len(action_padded))\n",
        "  action_1_test = np.append(action_1_test, int_data1, axis=0)\n",
        "  print(\"action_1_test\",len(action_1_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deJZgZHuXKJc",
        "outputId": "7fcba1fc-bcd4-4da5-a819-de6ab79384cc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "int-data 31\n",
            "action_padded 31\n",
            "action_1_test 31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_files = ['/content/drive/MyDrive/Datasets/Capstone/computer/12320.mp4'\n",
        "               ]\n",
        "action_2_test = np.empty((126)).reshape(1, -1)\n",
        "action_2_test = np.delete(action_2,np.s_[:],axis=0)\n",
        "for video in video_files:\n",
        "  int_data2 = gen_frames(video)\n",
        "  print(\"int-data\",len(int_data2))\n",
        "  action_padded = pad_sequences(int_data2, padding='post', maxlen=126, dtype='float32')\n",
        "  print('action_padded',len(action_padded))\n",
        "  action_2_test = np.append(action_2_test, action_padded, axis=0)\n",
        "  print(\"action_2_test\",len(action_2_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1QnRfqaXk2h",
        "outputId": "550872e3-ce5e-4c13-e642-c9addbc06349"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "int-data 31\n",
            "action_padded 31\n",
            "action_2_test 31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_files = ['/content/drive/MyDrive/Datasets/Capstone/drink/17723.mp4'\n",
        "               ]\n",
        "action_3_test = np.empty((126)).reshape(1, -1)\n",
        "action_3_test = np.delete(action_3,np.s_[:],axis=0)\n",
        "for video in video_files:\n",
        "  int_data3 = gen_frames(video)\n",
        "  print(\"int-data\",len(int_data3))\n",
        "  action_padded = pad_sequences(int_data3, padding='post', maxlen=126, dtype='float32')\n",
        "  print('action_padded',len(action_padded))\n",
        "  action_3_test = np.append(action_3_test, action_padded, axis=0)\n",
        "  print(\"action_3_test\",len(action_3_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCMcZnIVXq1J",
        "outputId": "f0f3877d-1f6b-41bf-8864-9c3a0378a35f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "int-data 31\n",
            "action_padded 31\n",
            "action_3_test 31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = np.append(action_1_test, action_2_test, axis=0)\n",
        "x_test = np.append(x_test, action_3_test, axis=0)"
      ],
      "metadata": {
        "id": "eXKHgMrSXu97"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Tdz49ajX0ON",
        "outputId": "c8adcb13-2779-462c-ad17-f530c2bcf1fd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(93, 126)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_y = []\n",
        "for i in range(0,3):\n",
        "  test_y = np.append(test_y,1).astype(int)\n",
        "for i in range(0,3):\n",
        "  test_y = np.append(test_y,2).astype(int)\n",
        "for i in range(0,3):\n",
        "  test_y = np.append(test_y,3).astype(int)\n",
        "test_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cEV9ETSYYUo",
        "outputId": "fd87f595-7a7b-4d42-f34f-8da208eb3346"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9,)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reshaping Training Data"
      ],
      "metadata": {
        "id": "PJ4pFR0TX7pH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.reshape(12,31,126)\n",
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KY-LYOiYDIA",
        "outputId": "a715d80f-ac55-41d6-a254-f4a3d7096c74"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12, 31, 126)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y = train_y.reshape(12,3)\n",
        "train_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnK6Slt1YJpc",
        "outputId": "1391b9a4-7097-409f-c024-cfb90bad1aa6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = x_test.reshape(3,31,126)\n",
        "x_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8EvCVGRYM26",
        "outputId": "0eacdc70-7c5a-4612-f157-be83abac8f84"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 31, 126)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_y = test_y.reshape(3,3)\n",
        "test_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDyF9B4fYQFb",
        "outputId": "43b227bb-2693-47ad-80f8-e9effeacb565"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Generation"
      ],
      "metadata": {
        "id": "bHPugw8tYhfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['chair','computer','drink']"
      ],
      "metadata": {
        "id": "-z3mhwCRYq52"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Activation\n",
        "from tensorflow.keras.callbacks import TensorBoard"
      ],
      "metadata": {
        "id": "2q2U_QtEYjYD"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len=31\n",
        "feature_len=126\n",
        "classes_len= len(x_train)\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, return_sequences=True, input_shape=(max_len, feature_len)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(LSTM(128, return_sequences=False))\n",
        "model.add(Dense(64))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9AGTzERYnCp",
        "outputId": "bc3e7ffa-55fe-4fa4-c83c-fd09563ff825"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_3 (LSTM)               (None, 31, 256)           392192    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 31, 256)           0         \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 31, 256)           525312    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 31, 256)           0         \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 128)               197120    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 64)               256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 64)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,123,331\n",
            "Trainable params: 1,123,203\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verbose, epochs, batch_size = 1, 25, 10\n",
        "model.fit(x_train, train_y, validation_data = (x_test, test_y),epochs=epochs, batch_size=batch_size, verbose=verbose, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeSuJBPfY31t",
        "outputId": "8cb5222c-7557-439e-be84-d3b614b83826"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 7.8351 - accuracy: 0.3333 - val_loss: 7.7247 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/25\n",
            "2/2 [==============================] - 1s 435ms/step - loss: 7.6922 - accuracy: 0.3333 - val_loss: 7.8024 - val_accuracy: 0.3333\n",
            "Epoch 3/25\n",
            "2/2 [==============================] - 0s 239ms/step - loss: 7.6798 - accuracy: 0.2500 - val_loss: 7.9483 - val_accuracy: 0.6667\n",
            "Epoch 4/25\n",
            "2/2 [==============================] - 0s 269ms/step - loss: 7.8362 - accuracy: 0.3333 - val_loss: 7.9602 - val_accuracy: 0.3333\n",
            "Epoch 5/25\n",
            "2/2 [==============================] - 0s 245ms/step - loss: 7.9185 - accuracy: 0.4167 - val_loss: 7.8941 - val_accuracy: 0.3333\n",
            "Epoch 6/25\n",
            "2/2 [==============================] - 0s 247ms/step - loss: 7.9098 - accuracy: 0.2500 - val_loss: 7.9899 - val_accuracy: 0.6667\n",
            "Epoch 7/25\n",
            "2/2 [==============================] - 0s 242ms/step - loss: 7.7817 - accuracy: 0.5000 - val_loss: 8.1213 - val_accuracy: 0.6667\n",
            "Epoch 8/25\n",
            "2/2 [==============================] - 0s 275ms/step - loss: 7.9217 - accuracy: 0.4167 - val_loss: 8.2445 - val_accuracy: 0.6667\n",
            "Epoch 9/25\n",
            "2/2 [==============================] - 1s 302ms/step - loss: 7.9995 - accuracy: 0.6667 - val_loss: 8.3236 - val_accuracy: 0.6667\n",
            "Epoch 10/25\n",
            "2/2 [==============================] - 0s 239ms/step - loss: 7.9357 - accuracy: 0.4167 - val_loss: 7.9195 - val_accuracy: 0.6667\n",
            "Epoch 11/25\n",
            "2/2 [==============================] - 0s 264ms/step - loss: 7.2814 - accuracy: 0.1667 - val_loss: 8.4779 - val_accuracy: 0.6667\n",
            "Epoch 12/25\n",
            "2/2 [==============================] - 0s 244ms/step - loss: 8.0247 - accuracy: 0.2500 - val_loss: 8.5579 - val_accuracy: 0.6667\n",
            "Epoch 13/25\n",
            "2/2 [==============================] - 0s 257ms/step - loss: 7.4266 - accuracy: 0.0833 - val_loss: 8.5140 - val_accuracy: 0.6667\n",
            "Epoch 14/25\n",
            "2/2 [==============================] - 0s 253ms/step - loss: 7.3876 - accuracy: 0.0000e+00 - val_loss: 8.3658 - val_accuracy: 0.6667\n",
            "Epoch 15/25\n",
            "2/2 [==============================] - 0s 286ms/step - loss: 7.4285 - accuracy: 0.3333 - val_loss: 7.9218 - val_accuracy: 0.6667\n",
            "Epoch 16/25\n",
            "2/2 [==============================] - 0s 265ms/step - loss: 7.5223 - accuracy: 0.1667 - val_loss: 7.0670 - val_accuracy: 0.6667\n",
            "Epoch 17/25\n",
            "2/2 [==============================] - 0s 240ms/step - loss: 7.7739 - accuracy: 0.5000 - val_loss: 7.1201 - val_accuracy: 0.6667\n",
            "Epoch 18/25\n",
            "2/2 [==============================] - 0s 247ms/step - loss: 8.3264 - accuracy: 0.5833 - val_loss: 7.2743 - val_accuracy: 0.6667\n",
            "Epoch 19/25\n",
            "2/2 [==============================] - 1s 461ms/step - loss: 8.2533 - accuracy: 0.6667 - val_loss: 7.4337 - val_accuracy: 0.6667\n",
            "Epoch 20/25\n",
            "2/2 [==============================] - 1s 418ms/step - loss: 8.2700 - accuracy: 0.6667 - val_loss: 7.5573 - val_accuracy: 0.6667\n",
            "Epoch 21/25\n",
            "2/2 [==============================] - 1s 438ms/step - loss: 8.2987 - accuracy: 0.5833 - val_loss: 7.6470 - val_accuracy: 0.6667\n",
            "Epoch 22/25\n",
            "2/2 [==============================] - 1s 447ms/step - loss: 7.8420 - accuracy: 0.5000 - val_loss: 7.7855 - val_accuracy: 0.6667\n",
            "Epoch 23/25\n",
            "2/2 [==============================] - 1s 434ms/step - loss: 7.8480 - accuracy: 0.5833 - val_loss: 7.8872 - val_accuracy: 0.6667\n",
            "Epoch 24/25\n",
            "2/2 [==============================] - 1s 607ms/step - loss: 8.1290 - accuracy: 0.6667 - val_loss: 7.9661 - val_accuracy: 0.6667\n",
            "Epoch 25/25\n",
            "2/2 [==============================] - 1s 321ms/step - loss: 8.0924 - accuracy: 0.5833 - val_loss: 8.0476 - val_accuracy: 0.3333\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa4aa7d19d0>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('action.h5')"
      ],
      "metadata": {
        "id": "X8Mqnta_ZRG3"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('action.h5')"
      ],
      "metadata": {
        "id": "WrwFG5fnZgin"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
      ],
      "metadata": {
        "id": "U2HW_s_xZii-"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = model.predict(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niihKfnvZlSu",
        "outputId": "f153c997-4c12-4153-eb61-a7d408ce1409"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ytrue = np.argmax(test_y, axis=1).tolist()\n",
        "yhat = np.argmax(yhat, axis=1).tolist()"
      ],
      "metadata": {
        "id": "HszdYP7PZmh6"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytrue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaNaU4bnZqK-",
        "outputId": "a054d6fe-bce3-4dd9-fcc2-988a539b7789"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yhat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1p-iqhaZsJR",
        "outputId": "45580bca-f5aa-4006-fe6b-ff0ec6dcc739"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 0, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multilabel_confusion_matrix(ytrue, yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ud4x15aDZvfF",
        "outputId": "56703ff7-7776-4046-d66d-c3360744b923"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0, 0],\n",
              "        [2, 1]],\n",
              "\n",
              "       [[1, 2],\n",
              "        [0, 0]]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(ytrue, yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNYld6hmZ41y",
        "outputId": "3ffec6da-ac14-4587-fc35-b21846282781"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    }
  ]
}